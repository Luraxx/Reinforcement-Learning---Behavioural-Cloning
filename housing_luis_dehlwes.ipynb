{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "t1",
   "metadata": {},
   "source": [
    "# Aufgabe 1 – Maschinelles Lernen mit dem California Housing Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importiern der benötigten Bibliotheken und des Datensatzes\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t2",
   "metadata": {},
   "source": [
    "## 1.1 Entscheidungsbaum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dateset laden und in X und Y aufteilen\n",
    "load_data = fetch_california_housing()\n",
    "X = load_data.data\n",
    "Y = load_data.target\n",
    "\n",
    "#Form überprüfen\n",
    "print(X.shape)\n",
    "\n",
    "#Trainings- und Testdaten aufteilen\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db296078",
   "metadata": {},
   "source": [
    "### 1.1.3 Art der ML-Aufgabe und mögliche Verfahren\n",
    "\n",
    "Art der Aufgabe: Es handelt sich um eine Regression-Aufgabe, da der Hauspreis eine kontinuierliche Zielvariable ist.\n",
    "\n",
    "**Fünf mögliche ML-Verfahren:**\n",
    "1. K-Nearest Neighbors Regressor\n",
    "2. Random Forest Regressor\n",
    "3. Support Vector Regression (SVR)\n",
    "4. Lineare Regression\n",
    "5. Neuronale Netzwerke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2193e52d",
   "metadata": {},
   "source": [
    "### 1.1.4. Funktion des Entscheidungsbaum\n",
    "An jedem Knoten stellt der Baum eine Frage: \"Ist Feature X größer als Schwellenwert Y?\" und teilt die Daten dann in zwei Gruppen.\n",
    "Den besten Schwellenwert findet er, indem er alle möglichen Splits ausprobiert und den nimmt der den MSE in den entstehenden Gruppen minimiert.\n",
    "Der Baum zerlegt den Datensatz Schritt für Schritt in immer kleinere Rechtecke im Feature-Raum, und in jedem Rechteck wird der Durchschnitt vorhergesagt. Der Baum wird so lange erweitert, bis ein Abbruchkriterium erreicht wird. Für die Vorhersage eines neuen Datenpunkts durch den Baum, wird der Datenpunkt von der Wurzel aus durch die Knoten geleitet, basierend auf den Antworten auf die Fragen an den Knoten, bis er ein Blatt erreicht. Das Blatt enthält die Vorhersage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55925084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entscheidungsbaum-Regressor erstellen und trainieren\n",
    "dt_model = DecisionTreeRegressor(random_state=41)\n",
    "dt_model.fit(X_train, Y_train)\n",
    "#Vorhersagen auf den Testdaten machen\n",
    "Y_pred = dt_model.predict(X_test)\n",
    "\n",
    "print(f'Anzahl der Blätter: {dt_model.get_n_leaves()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d373b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE berechnen\n",
    "mse_dt = mean_squared_error(Y_test, Y_pred)\n",
    "print(f'Mean Squared Error: {mse_dt:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0706eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nehme Spalte 1 von Data (print(load_data.feature_names))\n",
    "X_test_MedInc = X_test[:, 0]\n",
    "\n",
    "#Visualisierung\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X_test_MedInc, Y_test, label='Echte Werte', alpha=0.5, s=5)\n",
    "plt.scatter(X_test_MedInc, Y_pred, label='Vorhersage', alpha=0.5, s=5)\n",
    "plt.title('Echte Hauspreise vs. Vorhersagen des Entscheidungsbaums')\n",
    "plt.xlabel('Medianes Einkommen')\n",
    "plt.ylabel('Hauspreis')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301c2a09",
   "metadata": {},
   "source": [
    "Statt einer glatten Kurve sieht mam die Vorhersagen auf bestimmten Höhen, das sind die Durchschnittswerte der Blätter. Der Baum kennt nur endlich viele Ausgabewerte.\n",
    "Im Datensatz sind Preise über 5 abgeschnitten. Das sieht man als horizontale Linie bei y=5 bei den echten Werten. Problematisch für die Vorhersage. Es gibt Werte weit außerhalb des Bereichs, leichtes Overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t3",
   "metadata": {},
   "source": [
    "## 1.2 Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a225f68",
   "metadata": {},
   "source": [
    "**Vorteile:**\n",
    "- Bessere Generalisierung durch Aggregation mehrerer Bäume\n",
    "- Robust gegenüber Ausreißern und Rauschen\n",
    "- Kann komplexe nichtlineare Beziehungen modellieren\n",
    "\n",
    "**Nachteile:**\n",
    "- Weniger interpretierbar als einzelne Entscheidungsbäume\n",
    "- Kann bei sehr großen Datensätzen langsam sein\n",
    "- Kein einzelner Entscheidungspfad mehr nachvollziehbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Regressor erstellen und trainieren\n",
    "rf_model = RandomForestRegressor(random_state=41)\n",
    "rf_model.fit(X_train,Y_train)\n",
    "\n",
    "print(f\"Anzahl der Bäume: {rf_model.n_estimators}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd9f90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vorhersagen mit Random Forest machen\n",
    "Y_pred_rf_train = rf_model.predict(X_train)\n",
    "Y_pred_rf_test = rf_model.predict(X_test)\n",
    "\n",
    "#MSE und R2 Score berechnen\n",
    "mse_rf_train = mean_squared_error(Y_train, Y_pred_rf_train)\n",
    "mse_rf_test = mean_squared_error(Y_test, Y_pred_rf_test)\n",
    "\n",
    "r2_rf_train = r2_score(Y_train, Y_pred_rf_train)\n",
    "r2_rf_test = r2_score(Y_test, Y_pred_rf_test)\n",
    "\n",
    "#Ausgabe mit 4 Nachkommastellen\n",
    "print(f\"MSE Trainingsdaten: {mse_rf_train:.4f}\")\n",
    "print(f\"MSE Testdaten: {mse_rf_test:.4f}\")\n",
    "print(f\"R2 Trainingsdaten: {r2_rf_train:.4f}\")\n",
    "print(f\"R2 Testdaten: {r2_rf_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c92164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter-Tuning mit GridSearchCV\n",
    "\n",
    "#Mindestens 6 Kombinationen von n_estimators und max_depth\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None]\n",
    "}\n",
    "\n",
    "#GridSearchCV erstellen\n",
    "grid_search = GridSearchCV( \n",
    "    estimator=RandomForestRegressor(random_state=41),\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#GridSearchCV auf Trainingsdaten anwenden\n",
    "grid_search.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b25d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Beste Hyperparameter und MSE ausgeben\n",
    "print(f'Beste Hyperparameter: {grid_search.best_params_}')\n",
    "\n",
    "Y_pred_rf_best_train = grid_search.best_estimator_.predict(X_train)\n",
    "Y_pred_rf_best_test = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "print(f'MSE Trainingdaten: {mean_squared_error(Y_train, Y_pred_rf_best_train):.4f}')\n",
    "print(f'MSE Testdaten: {mean_squared_error(Y_test, Y_pred_rf_best_test):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6336b",
   "metadata": {},
   "source": [
    "### 1.2.6 Bootstraping und max features\n",
    "Bootstraping: Jeder Baum wird mit einem zufälligen Sample der Trainingsdaten trainiert. Dadurch entsteht Vielfalt unter den Bäumen, was die Generalisierung verbessert.\n",
    "\n",
    "Max features: Bei jedem Split wird nur eine zufällige Auswahl von Features berücksichtigt. Das erhöht die Diversität der Bäume weiter und verhindert, dass einzelne starke Prädiktoren alle Bäume dominieren. \n",
    "\n",
    "Durch diese Techniken sinkt die Varianz des Modells im Vergleich zu einem einzelnen Baum, während der Bias weitgehend gleich bleib."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t4",
   "metadata": {},
   "source": [
    "## 1.3 Neuronales Netzwerk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35568ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neuronales Netz erstellen und trainieren\n",
    "\n",
    "#Trainings-, Validierungs- und Testdaten aufteilen 70/15/15\n",
    "X_train_nn, X_temp, Y_train_nn, Y_temp = train_test_split(X, Y, test_size=0.3, random_state=41)\n",
    "X_val_nn, X_test_nn, Y_val_nn, Y_test_nn = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=41)\n",
    "\n",
    "#Neurales Netz mit Modell erstellen\n",
    "def build_nn_model(neurons, learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(8,))) #Eingabeschicht mit der Anzahl der Merkmale\n",
    "    model.add(Dense(neurons, activation='relu')) #Versteckte Schicht mit Relu-Aktivierungsfunktion\n",
    "    model.add(Dropout(0.2)) #Dropout-Schicht hinzufügen\n",
    "    model.add(Dense(1)) #Ausgabeschicht\n",
    "\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse', metrics=['mae'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979b3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter-Tuning für das Neuronale Netz - 6 verschiedene Kombinationen\n",
    "combinations = [\n",
    "    {'neurons': 32, 'learning_rate': 0.01},\n",
    "    {'neurons': 32, 'learning_rate': 0.001},\n",
    "    {'neurons': 64, 'learning_rate': 0.01},\n",
    "    {'neurons': 64, 'learning_rate': 0.001},\n",
    "    {'neurons': 128, 'learning_rate': 0.01},\n",
    "    {'neurons': 128, 'learning_rate': 0.001}\n",
    "]\n",
    "best_val_mae = float('inf')\n",
    "best_combo = None\n",
    "best_model = None\n",
    "\n",
    "#Training und Evaluierung für jede Kombination\n",
    "for combo in combinations:\n",
    "    neurons = combo['neurons']\n",
    "    learning_rate = combo['learning_rate']\n",
    "    print(f'Training mit {neurons} Neuronen und Lernrate {learning_rate}')\n",
    "\n",
    "    nn_model = build_nn_model(neurons, learning_rate)\n",
    "    history = nn_model.fit(X_train_nn, Y_train_nn, validation_data=(X_val_nn, Y_val_nn), epochs=100, batch_size=32, verbose=0)\n",
    "    \n",
    "    #Beste Kombination basierend auf Validierungsverlust speichern\n",
    "    val_mae = history.history['val_mae'][-1]\n",
    "    if val_mae < best_val_mae:\n",
    "        best_val_mae = val_mae\n",
    "        best_combo = combo\n",
    "        best_model = nn_model\n",
    "    \n",
    "print(f'Beste Kombination: {best_combo}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b25517c",
   "metadata": {},
   "source": [
    "### 1.3.4 Bedeutung Anzahl Neuronen in der Verdeckten Schicht und der learning rate\n",
    "Je mehr Neuronen, desto komplexere Funktionen kann das Netzwerk lernen. Aber zu viele Neuronen können auch zu Overfitting führen.\n",
    "\n",
    "Die learning rate bestimmt, wie stark die Gewichte bei jedem Schritt des Trainings angepasst werden. Eine zu hohe learning rate kann dazu führen, dass das Modell nicht konvergiert, während eine zu niedrige learning rate das Training sehr langsam macht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac59c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bestes Modell auf Testdaten evaluieren\n",
    "mae_train = best_model.evaluate(X_train_nn, Y_train_nn)[1]\n",
    "mae_val = best_model.evaluate(X_val_nn, Y_val_nn)[1]\n",
    "mae_test = best_model.evaluate(X_test_nn, Y_test_nn)[1]\n",
    "print(f'MAE Trainingsdaten: {mae_train:.4f}')\n",
    "print(f'MAE Validierungsdaten: {mae_val:.4f}')\n",
    "print(f'MAE Testdaten: {mae_test:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
